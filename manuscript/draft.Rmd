---
output:
  pdf_document: default
  html_document: default
---
# Moving towards large-scale data-intensive forecasting of species richness in breeding birds
# Using best practices for forecasting biodiversity changes in breeding birds
Authors: Harris, Taylor and White?

`r settings = yaml::yaml.load_file("settings.yaml")`

## Introduction

Forecasting the future state of ecological systems is increasingly important for
planning, management, and evaluating how well ecological models capture the key
ecological processes governing a system [@clark2001, @dietz2017]. One area of
particular interest for forecasting is biodiversity since it is important for
ecosystem function, central to conservation planning, and expected to be
influenced by anthropogenic global change. Anticipating potential changes in
biodiversity is crucial for large scale management and conservation, and for
addressing debates regarding whether local scale diversity is declining and may
continue to decline in response to anthropogenic influences [@dornelas2014].

Previous efforts to predict future patterns of species richness, and diversity
more generally, have focused primarily on building species distributions models
to quantify the spatial relationships between the occurrence of individual
species and the environment. Forecasts for future environmental conditions are
then used to predict where each species will occur in the future and the
probabilities of occurrence are summed across species to predict future species
richness [e.g., @thomas2004]. Alternatively, models that directly relate spatial
patterns of species richness to the environment have been developed and
generally perform equivalently to species distribution modeling based methods
[@algar2009, @distler2015].

However, despite the emerging interest in forecasting species richness and other
aspects of biodiversity, little is known about how effectively we can anticipate
their dynamics. This is due in part to the long time scales over which many
ecological forecasts are applied and the resulting difficulty in assessing
whether the predicted changes occurred. What we do know comes from a small
number of hindcasting studies, where models are built using data on species
occurrence and richness from the past and evaluated on their ability to predict
contemporary patterns [e.g., @algar2009, @distler2015]. These studies are a
valuable first step, but lack several components that are important for
developing good forecasting models and understanding how accurately these models
can predict the future. These components of good forecasting and evaluation (Box
1) broadly involve: 1) expanding the use of data to include biological and
environmental time-series [@treddenick2016]; 2) accounting for uncertainty in
observations, processes, model choice, and forecast evaluation
[@clark2001, @dietz2017]; and 3) conducting meaningful evaluations of the
forecasts by hindcasting, archiving short-term forecasts, and comparing
forecasts to baselines to determine if the forecasts are more accurate than
assuming the system is basically static [@Perretti2013].

In this paper we attempt to forecast the species richness of breeding birds at
hundreds of locations throughout North America while following the best
practices in Box 1 for ecological forecasting. To do this we combine 30 years of
time-series data on bird distributions with monthly time-series of climate data
and satellite-based remote-sensing. This 30 year time-scale has only recently
become possible for large-scale time-series based forecasting and allows us to
model and assess changes a decade or more into the future in the presence of
shifts in environmental conditions on par with predicted climate change. We
compare traditional distribution modeling based approaches to spatial models of
species richness, time-series methods, and simple baselines. All forecasts model
uncertainty and observation, are evaluated across different time lags using
hindcasting, and are publicly archived to allow future assessment. We discuss
the implications of these practices for our understanding of, and confidence in,
the resulting forecasts, and how we can continue to improve on these approaches
in the future.


## Methods

### Data

* BBS + retriever
* Predictor variables
  * Monthly climate data from interpolated weather stations (PRISM), converted
    to BIOCLIM variables. Many of these variables were tightly correlated, so
    we only included the 8 variables selected by Harris's (2015) BBS analysis
  * Weather/climate and NDVI were each calculated separately for each year,
    rather than averaged over the full multi-decade period (as Worldclim does)
  * NDVI (GIMMS)
  * Altitude (SRTM 90m Digital Elevation Database via raster::getData)
* Data summary: 
  * Number of sites, species, unique observers
  * central moments of richness (mean about 51, sd about 12 spp)
  * The within-site variation was much smaller (sd about 5 spp), making the
    site-level distributions underdispersed.  
    * This underdispersion implies
      that common count models such as binomial, Poisson, negative binomial 
      won't fit well.
    * For this reason (and because the time series methods assumed 
      Gaussian errors), we defined model performance throughout this paper in 
      terms of Gaussians (e.g. mean squared error), rather than with count
      distributions.
        * Some of the modeling approaches (SDMs and JSDMs) did use discrete
          distributions internally, however.
* Splitting the data temporally
  * Our data set began in `r settings$start_yr`, which was the first year that 
    NDVI data was available from GIMMS
  * All the models were trained on the data that was collected through 
    `r settings$last_train_year`
  * Sites were included in the analysis if there were at least 
    `r settings$min_num_yrs` years of data available during this period.
  * The models were tested/evaluated on the data from 
    `r settings$last_train_year + 1` `r settings$end_yr`.

  Bird species richness was compiled from the Breeding Bird Survey (BBS) [@pardieck2017] obtained using the Data Retriever software [@morris203]. The BBS data was initially filtered to exclude all nocturnal, cepuscular, and aquatic species (since these species are not well sampled by BBS methods), as well as unidentified species, and hybrids. All data from routes that did not meet BBS acceptable criteria were excluded. Only routes within the continental U.S.A were used so as to match the climatic dataset.  
  We used the 4 km resolution PRISM data [@prism] to calculate eight bioclimatic variables identified as relevant to bird distributions [@harris2015] for each year in the dataset: mean diurnal range, isothermality, max temperature of the warmest month, mean temperature of the wettest quarter, mean temperature of the driest quarter, precipitation seasonality, precipitation of the wettest quarter, precipitation of the warmest quarter. Satellite derived NDVI with an 8 km resolution was from the NDIV3g dataset [@pinzon2014] and was available from the time period 1981-2013. Elevation was from the SRTM 90m elevation dataset [@jarvis2008] obtained using the R package raster [@hijmans2014]. BBS route starting point coordinates were used to extract all environmental data. Climatic variables and NDVI were aggregated to the average values within a 40 km radius of the start point, equal to the length of a BBS route. NDVI was also subset to average summer (May, June, July) and winter (December, January, Feburary) values.
  Data from 1982 (the first complete year of NDVI data) to 2003 were used for training models and data from 2004 to 2013 was used for testing. Only BBS routes with at least 25 yearly observations in the years 1982-2015 were used.

### Accounting for site \& observer effects
* Observer effects are inherent in large data sets taken by different 
  observers, and are known to occur in BBS [citation].
* While these biases are known to exist, they are difficult to quantify: even if
  the different observers ran the same transects under identical conditions (which 
  they don't), we would still only have an estimate of their differences, not an 
  exact value.
* We estimated the observer effects, along with our uncertainty about those
  effects' size & direction, with a linear mixed model.
* This model partitions the variance in observed richness values into site-level
  variance, observer-level variance, and residual variance (e.g. variation within
  a site from year to year or from morning to morning).
  * No fixed effects in this model (e.g. environmental predictor variables because 
    we're not interested in explaining differences among sites at this point; we 
    just want to account for their existence while estimating observer effects.
  * Subtracting off the estimated observer effect provides an estimate of how many
    would have been found by a "typical" observer on that day; in this sense, it 
    enables us to correct for observer differences. We then used this "corrected" 
    version of the data for our time-series models such as the AR1, etc.
* A simple model like this one can't tell us exactly how each observer differed from
  the others, so it is important to represent our uncertainty about these differences.
  Here, we did so by collecting [[N]] Monte Carlo samples from the model's posterior
  distribution, using Stan. This provided us with [[N]] plausible values for each 
  observer, so we did not have to commit to a single value.
* While some methods for predicting richness might be able to use the uncertain
  estimates of the observer effects directly, we take the simpler approach of just
  fitting each of the downstream models with each of the Monte Carlo samples. This
  increases the amount of computational work by a factor of [[N]], but CPU time
  wasn't our limiting resource.

### Site-level models

[[note that these all use a Gaussian error distribution]]

**Baseline models.** The two simplest models we fit treated site-level richness
observations either as uncorrelated noise around a site-level constant (the 
"average" model) or as an autoregressive model with a single year of history 
(the "naive" model). The predictions from these two models are qualitatively 
different: predictions from the "average" model are centered on the average 
richness and the confidence intervals are narrow and constant-width. The
"naive" model, in contrast, predicts that future observations will be similar to
the final observed value (e.g. the value observed in `$r settings$last_train_year`), 
and the confidence intervals expand rapidly as the predictions extend farther into 
the future.  Each of these model types was fit separately to each BBS route in two
ways: first, we fit the model to the raw richness values, without accounting for
possible differences among observers at the site.  Then, we fit the models to the
residuals from our observer model (i.e. fitting the model to an estimate of how
many species a typical observer would have found, rather than to the raw data). 
In the latter case, we averaged across [[N]] models that were fit separately to 
the [[N]] Monte Carlo estimates of the observer effects, to account for our
uncertainty in the true values of those effects.

**Time series models.** Like the two baseline models, these models only looked at 
one site at a time, but they could include additional complexity. As above, these
models were trained both with the raw richness time series and with Monte Carlo
estimates of the observer-corrected time series. The  Auto-ARIMA models (based on 
Hyndman's 2016 `auto.arima` function) could include an autoregressive component 
(as in the "naive" model, but with the possibility of longer-term dependencies
in the underlying process), a moving average component (where the noise can have 
serial autocorrelation) and an integration/differencing component (so that the 
analysis could be performed on sequential differences of the raw data, rather than
on the observed time series itself). The `auto.arima` function chooses whether to 
include each of these components, and how many terms to include for each one, using
AICc, as described in Hyndman (2016). We used the default settings for this function,
apart from setting `seasonal = FALSE` because our time series does not include a 
seasonal component.  [[Note: should probably record somewhere what order model
the auto.arima function tends to choose.  Eyeballing the results, seems to be most 
similar to the naive model, but would be good to check.]] We also fit an 
"Auto-ARIMA + environment" model, in which each site-level time series could 
also be predicted by annual changes in our environmental variables at that site.

### Continental-scale models

So far, each model has been fit to one site at a time, with no information 
(apart from observer effects) shared among sites. When continental-scale data 
is available, ecologists often use correlative models to predict richness 
changes over time from environmental data. Here, we tested three variants of 
this approach. Here, we did *not* include the site-level random effects as 
predictors, meaning that this approach implicitly assumes that two sites 
with identical Bioclim, elevation, and NDVI values should have identical 
richness distributions. As above, we included observer effects & the 
associated uncertainty by running this model [[N]] times (once per MCMC 
sample).

**"Macroecological" model.** The most straightforward way to predict richness is 
to fit a richness model. We chose the `gbm` package for fitting boosted 
regression trees as our example of such a model. Boosted regression trees is a 
common approach in the species distribution modeling literature [refs] and 
works by fitting thousands of small tree-structured models sequentially, with
each tree optimized to reduce the error of its predecessors. For the reasons 
discussed above, this model was optimized using a Gaussian likelihood. [[other 
gbm settings go here.]] Compared to SDMs, this lets us predict richness directly 
instead of using hundreds of noisy species-level estimates, but it also discards 
any potentially-useful information regarding species turnover.

**SDMs.**
* Probably the most common approach?
* These models say that species' occurrence probabilities depend only on the values of
  the environmental variables at the time and place of the transect, not on the history
  of the environment or the history of the birds at that site
* Predict each species' occurrence probabilities individually, then add up the predictions
* [[Something about not using thresholds that throw away information]]
* Taking this approach in the face of uncertainty about observer effects would either 
  require a customized model (cite "neighborly advice" paper), or fitting [[N]] models
  times the number of species.
* To make it feasible to fit this large number of models, we used a custom wrapper 
  around the randomForest package. Random forests are flexible models that are built 
  by combining many independent sub-models ("trees") fit to different versions of the training
  data, which made it straightforward to adapt them to our situation: we simply fit one 
  tree using each of the Monte Carlo estimates of our observer effects and averaged the 
  predictions of the resulting forest.
* For the uncertainty, we used Gaussian approximations, based on the means and variances
  of the sums of independent Bernoulli random variables (cite Calabrese et al.).

**JSDM** 
* Joint species distribution model (JSDM) is a new approach that makes predictions
  about the full community composition, rather than modeling it species-by-species.
* In stacked single-species SDMs, the species-level occurrences are treated as 
  independent coin-flips whose proability depends only on the environment.
* JSDMs remove this independence assumption, and explicitly account for the
  possibility that a site will be much more (or less) suitable for birds in 
  general than one would expect based on a few environmental measurements alone.
* As a result, JSDMs do a better job of representing our uncertainty about 
  richness, while stacked SDMs underestimate it (Harris 2015).
* We used the `mistnet` package (Harris 2015) because it is the only JSDM that
  describes species' environmental associations with nonlinear functions.
* The mistnet package doesn't "know" about time series or spatial autocorrelation,
  so it did not share information among the repeated runs of the same transect.

### Ensembles
* Ensembles. Averaging predictions from multiple models tends to reduce the 
  noise in the estimates, and can thus lead to better predictions. Choosing the
  weights for the ensemble isn't straightforward because our estimates of model 
  error on the training set are biased in favor of models that
  overfit. [[Another option is to spatially cross-validate on the training set]]

### Model evaluation

* Evaluated each year between  `r settings$last_train_year` and `r settings$end_yr`
* When a model was run multiple times (e.g. once per MCMC sample for the observer
  estimates), we used the mean of those runs' point estimates as our final point 
  estimate and we calculated the uncertainty using the law of total variance (i.e.
  the average of the model runs' variance, plus the variance in the point estimates).
* Metrics:
  * (R)MSE 
    * How far, on average, are the models' predictions from the true value?
    * [[RMSE has better units than MSE (species versus squared species), but MSE is
      nice because it's additive (i.e. MSE of 2 is twice as big as MSE of 1).]]
  * Calibration/bias
    * In general, or in specific situations, is the model consistently predicting
      that richness will be too high or too low?
    * Can also report this value in terms of RMSE or MSE
  * Coverage of 95% confidence intervals
  * Gaussian deviance (simultaneously rewards good point estimates, precision, 
    and coverage)

Looked at how error changed as the time horizon of forecasting lengthened
* Error will generally tend to increase
* Error might flatten out after some number of years
* Error might increase at different rates for different methods
  * One model might be better at short time scales \& worse at long time scales

## Results

The mixed model (which formed the basis of our observer-corrected "average"
predictions) found that 70% of the variance in richness in the training set
could be explained by differences among sites, and 21% could be explained by
differences among observers. The remaining 9% represents residual variation,
where a given observer might report a different number of species on different
runs of the same transect. We found remarkably little autocorrelation in these
residuals ([[about 0.04, +/- something]]), suggesting richness is close to
stationary on these time scales, once observer differences are accounted for.

The substantial uncertainty associated with differences among observers caused
reliable improvements in both mean-squared-error and deviance when observer
effects were controlled for (Figure 4). This improvement was smallest for the
"Naive" model, as discussed below.

In general, each model's error increased more-or-less linearly with time,
although the slopes and initial error rates varied across models (Figure 3). The
site-level models (i.e. the baselines and the ARIMA methods) tended to do very
well (Figure 3), especially in terms of RMSE and especially on short time
horizons. For most combinations of year and metric, the best-performing model
was one of the two baselines. Usually, the "naive" model had the lowest error
when observer effects were not explicitly accounted for, and the "average" model
had the lowest error when they were. We discuss the reasons for this difference
in the next section.

Our two error metrics produced qualitatively similar results in most cases, with
the notable exception of the stacked SDMs (Figure 3A and 3B). Deviance, unlike
RMSE, depends on the full predictive distribution of a model, including the
model's estimated uncertainty. As noted elsewhere [@harris2015], these models
are generally far too confident, with observed richness values frequently
falling well outside their confidence limits (Figure 3C). As a result, the
stacked SDMs' deviance was dramatically higher than the next-worst model even
though its RMSE was not unusual. On the other hand, the richness-GBM model and
especially the mistnet JSDM had reasonably well-calibrated uncertainty
estimates, even in the presence of uncorrected observer biases (Figure 3C). This
prevented their deviance from rising as quickly as the site-level models when
the observer population changed (Figure 3B).

In general, the models produced confidence intervals that were too narrow,
indicating overconfident predictions. Of these, the random forest-based SDM
stack was the most overconfident, as discussed above. As one might expect,
correcting for observer differences improved the coverage of most models' 95%
intervals. Interestingly, this effect was very small for mistnet (its
coverage only improved from 92% to 93%), which seems to indicate that its
latent variables were already accounting for observer-related uncertainty. 
The "naive" model was the only model whose confidence intervals were too wide
(Figure 3), which can be attributed to the rapid rate at which
these intervals expand as the time lag increases (Figure 1).

## Discussion

Forecasting is an emerging imperative in ecology and as such the field needs to
develop and follow good practices for conducting and evaluating ecological
forecasts. We have laid out a number of these practices (Box 1) and attempted to
implement them in a single study that builds and evaluates forecasts of
biodiversity in the form of species richness. The results of this effort are
both promising and humbling. On an absolute basis, when comparing forecasts
across sites, many different approaches to forecasting produce reasonable
forecasts. If a site is predicted to have a high number of species in the
future, relative to other sites, it generally does. However, within a site the
forecasts perform relatively poorly. If a site is predicted to have a higher
richness next year than it did last year there is little confidence that this
will be the case. This results in simple site-level time-series and baseline
models like the long-term average and the naive model being some of the best
performing overall models for future biodiversity.

The most commonly used method to forecast the future state of biodiversity,
fitting a species distribution model separately to each species and then summing
the probability of presence across those models, provides the worst overall
forecasts of all methods evaluated. Most forecasting based on this approach is
never evaluated and when it is it is typically only assessed based on cross-site
rather than within site approaches. As a result the forecasts may appear to
perform well even when change through time is not accurately predicted. This
issue has also recently been recently highlighted as an issue for using SDMs to
predict the future locations of individual species [plos one 2012 paper]. This
result is particularly sobering because this method forms the foundation for
major claims regarding the predicted loss of biodiversity to climate change
[2004 science/nature papers]. The poor performance of SDM-based forecasts
highlights the crucial importance of comparing multiple modeling approaches when
conducting ecological forecasts, and in particular the value of comparing
results to simple baselines to avoid over-interpreting the accuracy of
ecological forecasts. Disciplines that have more mature forecasting cultures
often do this by reporting "forecast skill", which is the improvement in the
forecast relative to a simple baseline. We recommend adopting this approach in
future ecological forecasting research. It also highlights the need for using
hindcasting and time-series data for assessing the effectiveness of forecasting
methods, since this allows comparisons to baselines and assessment of the
predicted dynamics at each site.

The most common alternative to summed SDMs is modeling richness directly using
environmental variables and using the resulting models to make forecasts [].
Two comparisons of the the SDM and direct richness modeling approaches reported
that the methods yielded equivalent results for forecasting diversity. While our
results generally support this for point estimates of richness they also show
that modeling richness directly results in much better estimation of the
uncertainty of the forecast and therefore is a better overall approach to
forecasting richness. A similar result is seen when comparing joint species
distribution models (JSDMs) to stacked single species distribution models. For
point estimates the joint distribution models are roughly equivalent to stacked
SDMs, but the JSDMs provide much better estimates of uncertainty. In fact, JSDMs
and modeling richness directly provide some of the best estimates of uncertainty
across all modeling approaches. This highlights the importance evaluating models
based on their uncertainty as well as their mean estimates. Not doing so is
particularly problematic in this case because the summed species distribution
models are overly confident meaning that using them would lead to believing that
richness would be restricted to a much narrower range than would actually
occur.

The accurate estimates of uncertainty by JSDMs and richness models result in
these models approaching the performance of time-series models and baselines at
the longest time-scales assessed when using measures that consider both point
estimates and uncertainty. This convergence at longer time scales highlights the
importance of considering how forecasting method performance changes with the
distance into the future being predicted. The value of this approach, and
associated forecast horizons, has been raised for knowing how far into the
future a model can be effectively used for forecasting [@petchy2015], but our
results suggest a broader value to this approach for considering the potential
importance of different models and processes for making forecasts at different
scales. [[Should make sure Petchy et al. don't make this point]] Our results
show that for forecasts 1-3 years in the future, baselines and time-series based
approaches outperform all other methods using integrative metrics. At these time
scales environmental changes are relatively small and changes in the biota may
lag behind changes in the environment. However, as the timescale of the forecast
increases to a decade the JSDMs and environmental richness models get closer to
the accuracy of the time-series methods and even surpass some of those
methods. As the forecast distance increases the amount of environmental change
is expected to increase and the system will have more time to respond,
potentially leading to shifts towards improved relative performance of the
models incorporating environmental and ecological information. If this shift
with forecast distance in the relative importance of different processes and
models continues, this could lead to JSDMs and environmental richness models
outperforming time-series based approaches at sufficiently long time
scales. These results suggest that assessment of how forecast performance
changes with time lag is not only important for determining how far in the
future to forecast, but also for determining models and processes are most
relevant for making forecasts at different time scales.

It is also possible that models that include species' relationships to their
environments or direct environmental constraints on richness may continue to
produce forecasts that are worse than simply assuming the systems are
static. This would be expected to occur is the systems are in fact not changing
their richness over the relevant time scales and therefore simple models of no
directional change are appropriate. Recent suggestions that local scale richness
in some systems is not changing directionally at multi-decadal scales supports
this possibility [@brown2001; @ernest2001; @dornelas201X; @theplantone]. This
lack of change may be expected even in the presence of substantial changes in
environmental conditions and species composition at a site due to the
replacement of species from the regional pool [@brown2001; @ernest2001]. On
average the Breeding Bird Survey sites used in this study show little change in
richness ([[Add summary stat here]]) [see also @lasorte2007]. This absence of
rapid change is beneficial for the absolute accuracy of forecasts across sites,
because if a past years richness is known it is easy to estimate a future
richness, and explains why stacked SDMs performs relatively well at this task
despite failing to capture meaningful dynamics. However it makes it difficult to
improve forecasts relative to simple baselines, since those baselines are close
to representing what is actually occurring in the system. This suggests that
simple time-series models and baselines should be actively considered for
forecasts of richness and other stable aspects of biodiversity and also suggests
that future efforts to understand and forecast biodiversity should also focus on
composition since it is expected to be more dynamic
[@ernest2001; @dornelas201X].

In addition to consideration of the different process models used for
forecasting it is important to consider the observation models. When working
with any ecological dataset there are imperfections in sampling that have the
potential to influence results. With large scale survey and citizen science
datasets like the Breeding Bird Survey these issues are potentially magnified by
the large number of different observers and major differences in habitat and
species. We included an observation model for one of the two major observation
issues known for the Breeding Bird Survey, differences among
observers. Accounting for differences in observers substantially reduced the
error in point estimates in all models and also improved the coverage of the
confidence intervals. In addition, it resulted in changes in which models
performed best, most notably reducing the performance of the naive model for
point estimates. This suggests that the naive model performed well in part
because it was capable of accommodating rapid shifts in estimated richness
introduced by changes in the observer. These kinds of rapid changes were
difficult for the other time-series models to accommodate and so the ARIMA and
average models improved substantially once this source of observation error was
addressed. This demonstrates that properly modeling observation error can be
important for reducing uncertainty in forecast but can also lead to changes in
the best methods for forecasting. We did not address differences in detection
probability across species and sites since there is no clear way to address this
issue without making strong assumptions about the data, but this would be a
valuable addition to future forecasting models.

Future efforts will also need to begin to address the additional uncertainty
that comes from error in forecasting the environmental conditions themselves. In
this and other hindcasting studies the environmental conditions for the "future"
are known because the data has already been observed. In real forecasts the
environmental conditions themselves have to be forecast and those forecasts have
uncertainty and bias. This will cause ecological forecasts that use
environmental data to be more uncertain. It is important to correctly
incorporate this uncertainty in the predictor variables into forecasting models
[@clark2001; @dietz2017]. Difficulty in forecasting future environmental
conditions at small scales will present continued challenges for models
incorporating these conditions and this may result in a continued advantage to
simple time-series based approaches.

The science of forecasting biodiversity, and ecology more broadly, remains in
its infancy and it is important to consider the general inability the
forecasting methods to improve on simple baselines in that context. When weather
forecasting first started the forecasts were likewise worse than simple
baselines []. One of the things that helped weather forecasts improve was large
numbers of forecasts were made in public, which allowed different approaches to
forecasting to be regularly assessed and improved
[@mcgill2012 blog post; history of weather forecasting book]. This suggests that
it is important for ecologists to start regularly making and evaluating true
ecological forecasts, even if they perform poorly, and to make these forecasts
publicly available for assessment. These forecasts should include both
short-term predictions, which can be assessed quickly, and mid to long-term
forecasts to help assess long time-scale processes and determine how far into
the future we can successfully forecast [@tredenick ???, @dietz2017]. Forecasts
for the next XX years for the models in this paper are openly archived on Dryad
(XXXXXXXXXXX) so that we and others can assess how well they perform and we plan
to evaluate these forecasts and report the results as each new year of BBS data
becomes available. Weather forecasting has continually improved throughout its
history [@bauer2015] in part due to making and evaluating public forecasts
[@mcgill2012] and we hope this will help ecology do the same.

Making successful ecological forecasts will be challenging. Ecological systems
are complex, fundamental theory is less refined than simpler physical/chemical
systems, and we currently lack the kinds of truly "big" data that produce
effective forecasts through machine learning. Despite this we believe that
progress can be made if we build an active forecasting culture in ecology that
builds and assesses forecasts in ways that will allow us to improve the
effectiveness of ecological forecasts most rapidly (Box 1). This includes
expanding the scope of the ecological and environmental data we work with,
paying attention to uncertainty in both model building and forecast evaluation,
and rigorously assessing forecasts using a combination of hindcasting, archived
forecasts, and comparisons to simple baselines.


## Box 1: Ten simple rules for making and evaluating ecological forecasts

### 1. Compare multiple modeling approaches

Typically ecological forecasts use one modeling approach or a small number of
related approaches. By fitting and evaluating multiple modeling approaches we
can learn more rapidly about the best approaches for making predictions for a
given ecological quantity. This includes comparing process based and data-driven
models and comparing the accuracy of forecasts to simple baselines to determine
if the modeled forecasts are more accurate than the naive assumption that the
world is static.

### 2. Use time-series data when possible

Forecasts describe how systems are expected to change through time. While some
areas of ecological forecasting focus primarily on time-series data, others
primarily focus on using spatial models and space-for time substitutions. Using
ecological and environmental time-series data allows the consideration of actual
dynamics from both a process [@treddenick2016a] and error structure perspective.

### 3. Pay attention to uncertainty

Understanding confidence in a forecast is just as important as understanding the
average or expected outcome. Failing to account for uncertainty can result in
overconfidence in highly uncertain outcomes leading to poor decision making and
erosion of confidence in ecological forecasts. Models should explicitly include
sources of uncertainty and how the propagate through the forecast where
possible. Evaluations of forecasts should assess the accuracy of uncertainties
as well as point estimates [@dietz2017].

### 4. Use predictors related to the question

Many ecological forecasts use data that is readily available and easy to work
with. While ease of use is a reasonable consideration it is also important to
include predictor variables that are expected to relate to the ecological
quantity being forecast and dynamic time-series of predictors instead of
long-term averages. Investing time in identifying and acquiring better predictor
variables may have at least as many benefits as using more sophisticated
modeling techniques.

### 5. Assess how forecast accuracy changes with time-lag

In general the accuracy of forecasts decreases with the length of time into the
future being forecast [@petch2015]. This decay in accuracy and the potential for
different rates of decay to result in different relative model performance at
different lead times should be considered when evaluating forecasts and
comparing models.

### 6. Include an observation model

Ecological observations are influenced by both the underlying processes and how
the system is sampled. When possible forecasts should model the factors
influencing the observation of the data.

### 7. Validate using hindcasting

To evaluate the expected accuracy and uncertainty of forecasts assess the
performance of these forecasts within existing time-series data.

### 8. Publicly archive forecasts

To allow the future evaluation of the accuracy and uncertainty of forecasts the
forecast values and/or models should be archived so that they can be assessed
after new data is generated [@mcgill2012]. Enough information should be provided
to allow an unambiguous assessment of the forecast performance.

### 9. Make short-term and long-term predictions

In cases where long-term predictions are the primary goal, short-term should
also be made to accommodate the time-scales of planning and management decisions
and to allow the accuracy of the forecasts to be quickly evaluated
[@treddenick2016a].

### 10. [[?]]

Sites' biotic and abiotic environments differ in more ways than the 
few ones you happened to measure. As a result, some sites will 
consistently have more species than a regression model (or sum of 
regression models) would predict, while others willl consistently have 
fewer. Depending on the spatial scale of these unmeasured variables, 
independent site-level random effects could be sufficient for dealing 
with this, or spatially-autocorrelated random effects might be needed.
